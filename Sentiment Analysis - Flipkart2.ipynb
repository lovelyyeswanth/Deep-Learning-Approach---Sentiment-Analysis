{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f371b76",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff00f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id                                             Review  Label\n",
      "0   0                                   good interesting      5\n",
      "1   1  class helpful currently im still learning clas...      5\n",
      "2   2  likeprof tas helpful discussion among students...      5\n",
      "3   3  easy follow includes lot basic important techn...      5\n",
      "4   4      really nice teacheri could got point eazliy v      4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Join the words back into a cleaned sentence\n",
    "    cleaned_text = ' '.join(words)\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the clean_text function to the 'Review' column\n",
    "df['Review'] = df['Review'].apply(clean_text)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee10006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7689216968790881\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.36      0.43       493\n",
      "           2       0.41      0.11      0.17       484\n",
      "           3       0.29      0.09      0.14       933\n",
      "           4       0.49      0.16      0.24      3613\n",
      "           5       0.80      0.98      0.88     15881\n",
      "\n",
      "    accuracy                           0.77     21404\n",
      "   macro avg       0.50      0.34      0.37     21404\n",
      "weighted avg       0.71      0.77      0.71     21404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust the max_features\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')  # You can choose different kernels like 'linear', 'rbf', etc.\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bc22a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.8921696879088"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "accuracy_score(y_pred,y_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23474d70",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea54e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1204/1204 [==============================] - 14s 11ms/step - loss: 0.6633 - accuracy: 0.7600 - val_loss: 0.5995 - val_accuracy: 0.7761\n",
      "Epoch 2/10\n",
      "1204/1204 [==============================] - 13s 11ms/step - loss: 0.5533 - accuracy: 0.7900 - val_loss: 0.6108 - val_accuracy: 0.7730\n",
      "Epoch 3/10\n",
      "1204/1204 [==============================] - 13s 11ms/step - loss: 0.4648 - accuracy: 0.8276 - val_loss: 0.6684 - val_accuracy: 0.7696\n",
      "Epoch 4/10\n",
      "1204/1204 [==============================] - 13s 11ms/step - loss: 0.3519 - accuracy: 0.8722 - val_loss: 0.7781 - val_accuracy: 0.7516\n",
      "669/669 [==============================] - 1s 2ms/step\n",
      "Accuracy of CNN is 0.7512614464586058\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"reviews.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        words = word_tokenize(text)\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "        cleaned_text = ' '.join(words)\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "data['Tidy_Reviews'] = data['Review'].apply(preprocess_text)\n",
    "\n",
    "def convert_text_to_numerical(text):\n",
    "    unique_words = len(set(' '.join(data['Tidy_Reviews']).split()))\n",
    "    num_words = min(7000, unique_words)\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    sequence_lengths = [len(seq) for seq in sequences]\n",
    "    average_length = int(sum(sequence_lengths) / len(sequence_lengths))\n",
    "    maxlen = min(140, average_length * 2)\n",
    "    pad_seqs = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "    return pad_seqs, tokenizer, maxlen, num_words  # Include num_words in the return values\n",
    "\n",
    "data = data.reset_index()\n",
    "numeric_reviews, tokenizer, maxlen, num_words = convert_text_to_numerical(data['Tidy_Reviews'])\n",
    "data.insert(len(data.columns)-1, \"numeric_reviews\", numeric_reviews.tolist())\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data.insert(len(data.columns), \"encoded_labels\", label_encoder.fit_transform(data['Label']))\n",
    "\n",
    "# Splitting the dataset\n",
    "inputs = data[['numeric_reviews']]\n",
    "outputs = data[['encoded_labels']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "X_train = np.asarray(X_train['numeric_reviews'].tolist(), dtype=np.int32)\n",
    "X_test = np.asarray(X_test['numeric_reviews'].tolist(), dtype=np.int32)\n",
    "y_train = np.asarray(y_train['encoded_labels'].tolist(), dtype=np.int32)\n",
    "y_test = np.asarray(y_test['encoded_labels'].tolist(), dtype=np.int32)\n",
    "\n",
    "# Building the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words, output_dim=128, input_length=maxlen, trainable=True))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))  # Assuming you have 6 classes\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.1)])\n",
    "\n",
    "# Evaluation\n",
    "predicted_labels = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predicted_labels, axis=1)\n",
    "\n",
    "cnn_accuracy = accuracy_score(y_test, predicted_classes)\n",
    "print(\"Accuracy of CNN is\", cnn_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ede9ec",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0277048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1204/1204 [==============================] - 32s 25ms/step - loss: 0.6814 - accuracy: 0.7564 - val_loss: 0.6078 - val_accuracy: 0.7745\n",
      "Epoch 2/10\n",
      "1204/1204 [==============================] - 30s 25ms/step - loss: 0.5913 - accuracy: 0.7797 - val_loss: 0.6016 - val_accuracy: 0.7734\n",
      "Epoch 3/10\n",
      "1204/1204 [==============================] - 33s 28ms/step - loss: 0.5619 - accuracy: 0.7908 - val_loss: 0.6160 - val_accuracy: 0.7717\n",
      "Epoch 4/10\n",
      "1204/1204 [==============================] - 34s 29ms/step - loss: 0.5372 - accuracy: 0.8001 - val_loss: 0.6287 - val_accuracy: 0.7701\n",
      "Epoch 5/10\n",
      "1204/1204 [==============================] - 37s 31ms/step - loss: 0.5126 - accuracy: 0.8093 - val_loss: 0.6365 - val_accuracy: 0.7724\n",
      "669/669 [==============================] - 3s 4ms/step\n",
      "Accuracy of LSTM is 0.7737338815174734\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"reviews.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        words = word_tokenize(text)\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "        cleaned_text = ' '.join(words)\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "data['Tidy_Reviews'] = data['Review'].apply(preprocess_text)\n",
    "\n",
    "def convert_text_to_numerical(text):\n",
    "    unique_words = len(set(' '.join(data['Tidy_Reviews']).split()))\n",
    "    num_words = min(7000, unique_words)\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    sequence_lengths = [len(seq) for seq in sequences]\n",
    "    average_length = int(sum(sequence_lengths) / len(sequence_lengths))\n",
    "    maxlen = min(140, average_length * 2)\n",
    "    pad_seqs = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "    return pad_seqs, tokenizer, maxlen, num_words  # Include num_words in the return values\n",
    "\n",
    "data = data.reset_index()\n",
    "numeric_reviews, tokenizer, maxlen, num_words = convert_text_to_numerical(data['Tidy_Reviews'])\n",
    "data.insert(len(data.columns)-1, \"numeric_reviews\", numeric_reviews.tolist())\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data.insert(len(data.columns), \"encoded_labels\", label_encoder.fit_transform(data['Label']))\n",
    "\n",
    "# Splitting the dataset\n",
    "inputs = data[['numeric_reviews']]\n",
    "outputs = data[['encoded_labels']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "X_train = np.asarray(X_train['numeric_reviews'].tolist(), dtype=np.int32)\n",
    "X_test = np.asarray(X_test['numeric_reviews'].tolist(), dtype=np.int32)\n",
    "y_train = np.asarray(y_train['encoded_labels'].tolist(), dtype=np.int32)\n",
    "y_test = np.asarray(y_test['encoded_labels'].tolist(), dtype=np.int32)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words, output_dim=128, input_length=maxlen, trainable=True))\n",
    "model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(6, activation='softmax'))  # Assuming you have 6 classes\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.001)])\n",
    "\n",
    "# Evaluation\n",
    "predicted_labels = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predicted_labels, axis=1)\n",
    "\n",
    "lstm_accuracy = accuracy_score(y_test, predicted_classes)\n",
    "print(\"Accuracy of LSTM is\", lstm_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
